import scrapy, html
import time

from scrapy.crawler import CrawlerProcess
#from item_cve import CveItem 
from multiprocessing import Queue
from multiprocessing import Process, get_context

from scrapy import signals
#from scrapy.conf import settings
from scrapy.crawler import CrawlerProcess
#from scrapy.xlib.pydispatch import dispatcher

import requests
from bs4 import BeautifulSoup
from lxml import etree
import json

from bs4 import BeautifulSoup
from lxml import html
import requests
import time
from utils_DATA import encode_content
import os
import re

#date format changer
from datetime import datetime

import sys


def crawl_content_from_link(link):
    page = ''
    idx = 0
    while page == '':
        if idx > 10:
            return
        try:
            idx += 1
            page = requests.get(link, timeout=60, headers={'User-Agent': "Magic Browser"})
            break
        except requests.exceptions.HTTPError as errh:
            print("Http Error: " + str(errh) + " Please check: " + link)
        except requests.exceptions.ConnectionError as errc:
            print("Error Connecting:" + str(errc) + " Please check: " + link)
            time.sleep(5)
            print(idx, "Was a nice sleep, now let me continue...")
            continue
        except requests.exceptions.Timeout as errt:
            print("Timeout Error:" + str(errt) + " Please check: " + link)
        except requests.exceptions.RequestException as err:
            print("Other errors!" + str(err) + " Please check: " + link)
    time.sleep(10)
    return page.content, BeautifulSoup(page.content, features="lxml").get_text()


def parse_edb(raw_content, report_link):
    tree = html.fromstring(raw_content)
    keyword_section = tree.xpath('/html/body/div/div[2]/div[2]/div/div/div[1]/div/div[1]/h1/text()')
    if len(keyword_section) > 0:
        title = keyword_section[0].replace('\n', ' ')
        cve_line = tree.xpath('/html/body/div/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[1]/div/div[1]/div/div/div/div[2]/h6/a/text()')
        edb_line = tree.xpath('/html/body/div/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[1]/div/div[1]/div/div/div/div[1]/h6/text()')
        date_line = tree.xpath('/html/body/div/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[3]/div/div[1]/div/div/div/div[2]/h6/text()')
        platrform_line = tree.xpath('/html/body/div/div[2]/div[2]/div/div/div[1]/div/div[2]/div[1]/div[3]/div/div[1]/div/div/div/div[1]/h6/a/text()')
        #print(cve_line)
        
        link = report_link.replace('/exploits/', '/raw/')
        page = requests.get(link, timeout=60, headers={'User-Agent': "Magic Browser"})

        crawl_result = crawl_content_from_link(link)
        if crawl_result is None:
            return {}
        str_content, _ = crawl_result

        str_content = str(page.content).replace('\r\n', ' ')

        #created date check in edb page (content)
        date_match = re.search(r'Date: (\d{2}/\d{2}/\d{4})', str_content)
        created = "NA"
        if date_match:
            created = date_match.group(1)
            #date format
            date_obj = datetime.strptime(created, "%d/%m/%Y")
            created = date_obj.strftime("%d-%b-%Y")
        else:
            created = "NA"

        #date format
        edb_published = encode_content(date_line[0])
        date_obj = datetime.strptime(edb_published, "%Y-%m-%d")
        edb_published = date_obj.strftime("%d-%b-%Y")
        #exploit-timestamps
        dates = ({"created": created, "edb-published": edb_published})

        if len(cve_line) > 0:
            cve_line = encode_content(cve_line[0])
            for i in cve_line:
                if not (i.isdigit() or i == '-'):
                    cve_line = ''

            return {'cve-id': ['CVE-' + cve_line], 'title': encode_content(title), 'content': [encode_content(str_content)], 'edb_id': encode_content(edb_line[0]), "exploit-timestamps": dates, "platform": encode_content(platrform_line[0])}
        else:
            return {'cve-id': [], 'title': encode_content(title), 'content': [encode_content(str_content)], 'edb_id': encode_content(edb_line[0]), "date": encode_content(date_line[0]),  "date_created": created, "platform": encode_content(platrform_line[0])}
    print('ERROR in parse_edb')
    return {}


cve_map = open('CVE_edb_map.json', encoding='utf-8')
map = json.load(cve_map)

#running file individually
#input_name = input()

# taking input from "automated_vesData.py"
input_name = sys.argv[1]

judge_file_exist = os.path.exists("./results/"+input_name+'_CVEs.json')
if judge_file_exist:
    cve_store = open("./results/"+input_name+'_CVEs.json', encoding='utf-8')
    data = json.load(cve_store)
else:
    print("Not exist such file!")
    exit(1)

new_dct = {}
for cve in data["ves"]["ves:vulnerability"]: 
    
    if cve in map.keys():
        print(cve)
        for i in map[cve]:
            report_link = 'https://www.exploit-db.com/exploits/'+i
            crawl_result = crawl_content_from_link(report_link)
            raw_content, clean_content = crawl_result
            target_content_dic = parse_edb(raw_content, report_link)
           
            new_dct[target_content_dic['edb_id']] = {}
            new_dct[target_content_dic['edb_id']]["exploit-title"] = target_content_dic['title']
            new_dct[target_content_dic['edb_id']]["platform"] = target_content_dic['platform']
            new_dct[target_content_dic['edb_id']]["exploit-timestamps"] = target_content_dic['exploit-timestamps']
            new_dct[target_content_dic['edb_id']]["cve"] = target_content_dic['cve-id']
            new_dct[target_content_dic['edb_id']]["content"] = target_content_dic['content']
            #cve['exploit title'].append(target_content_dic['title'])
            #cve['content'].append(target_content_dic['content'])
            #print(target_content_dic)
    data["ves"]["ves:exploit"] = new_dct
    with open("./results/"+input_name+"_data.json", 'w') as fp:
        json.dump(data, fp, indent=4)
            

